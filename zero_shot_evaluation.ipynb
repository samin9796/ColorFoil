{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# install the required libraries\n",
        "\n",
        "!pip install transformers\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "koWK0A_NDd3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import webcolors\n",
        "import random\n",
        "\n",
        "# preparing the ColorFoil benchmark\n",
        "\n",
        "def prapare_data():\n",
        "\n",
        "  img_list = [] # list of image urls\n",
        "  cap_list = [] # list of captions\n",
        "  foil_list = [] # list of foiled captions\n",
        "\n",
        "  with open('/content/captions_val2017.json') as f: # read the data\n",
        "      d = json.load(f)\n",
        "      img_length = len(d[\"images\"]) # total number of images in the MS COCO val set (2017)\n",
        "\n",
        "      # create three lists of images, captions and foils\n",
        "      for i in d[\"images\"]:\n",
        "        id = i[\"id\"]\n",
        "        flag = False\n",
        "        for j in d[\"annotations\"]:\n",
        "          if j[\"image_id\"]==id:\n",
        "            caption = j[\"caption\"]\n",
        "            for word in caption.split(' '):\n",
        "              if word in webcolors.CSS3_NAMES_TO_HEX: # using the webcolor python package\n",
        "                flag = True\n",
        "            if flag == True:\n",
        "              foil = create_foil(caption) # call the create_foil function. it will randomly choose a foil color.\n",
        "              img_list.append(i[\"coco_url\"])\n",
        "              cap_list.append(caption)\n",
        "              foil_list.append(foil)\n",
        "              print(i[\"coco_url\"])\n",
        "              print(caption, foil)\n",
        "              break\n",
        "\n",
        "      return img_list, cap_list, foil_list\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "9vsqHa30T0Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace the original color with the foil color\n",
        "\n",
        "def create_foil(caption):\n",
        "  # most commonly used colors\n",
        "  colors = [\"blue\", \"black\", \"red\", \"pink\", \"yellow\", \"grey\", \"orange\", \"white\", \"green\", \"brown\"]\n",
        "  lst = caption.split(' ')\n",
        "  for color in lst:\n",
        "    if color in webcolors.CSS3_NAMES_TO_HEX:\n",
        "      num = random.randint(0, 9)\n",
        "      if colors[num] == color:\n",
        "        foiling_color = colors[num-1]\n",
        "      else: foiling_color = colors[num]\n",
        "      caption = caption.replace(color, foiling_color)\n",
        "  return caption\n"
      ],
      "metadata": {
        "id": "vnkNHDXhvBCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finally we have the three lists\n",
        "\n",
        "img_list, cap_list, foil_list = prapare_data()"
      ],
      "metadata": {
        "id": "_QdDbXJPuccH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP Model"
      ],
      "metadata": {
        "id": "1wghld_ab0qh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HpjLEFvDQdA"
      },
      "outputs": [],
      "source": [
        "'''zero-shot evaluation of CLIP on the ColorFoil '''\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from transformers import ViltProcessor, ViltModel\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "model.to(device)\n",
        "\n",
        "acc = 0 # number of correct caption-foil pairs\n",
        "\n",
        "for i in tqdm(range(len(img_list))):\n",
        "  url = img_list[i]\n",
        "\n",
        "  if i == 550 or i == 1164 or i==1752 or i ==2236: # these are greyscale images. we are removing them\n",
        "    continue\n",
        "\n",
        "  image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "  inputs = processor(text=[cap_list[i], foil_list[i]], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "  outputs = model(**inputs)\n",
        "  logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "  probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
        "\n",
        "  if probs[0][0] > probs[0][1]:\n",
        "    acc += 1\n",
        "\n",
        "\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calcualte the final accuracy\n",
        "\n",
        "print(acc)\n",
        "print(len(img_list)-4)\n",
        "print('The accuracy is: ', acc/(len(img_list)-4)*100) # substitute the grayscale images"
      ],
      "metadata": {
        "id": "C-wDeNiicoIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViLT Model"
      ],
      "metadata": {
        "id": "qDf9Xy4zdTOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViltProcessor, ViltForImageAndTextRetrieval\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-coco\")\n",
        "model = ViltForImageAndTextRetrieval.from_pretrained(\"dandelin/vilt-b32-finetuned-coco\")\n",
        "model.to(device)\n",
        "\n",
        "acc = 0\n",
        "\n",
        "for i in tqdm(range(len(img_list))):\n",
        "  url = img_list[i]\n",
        "  if i == 550 or i == 1164 or i==1752 or i==2236: # these are greyscale images. we are removing them\n",
        "    continue\n",
        "  image = Image.open(requests.get(url, stream=True).raw)\n",
        "  texts = [cap_list[i], foil_list[i]]\n",
        "\n",
        "  # forward pass\n",
        "  scores = dict()\n",
        "  for text in texts:\n",
        "      # prepare inputs\n",
        "      encoding = processor(image, text, return_tensors=\"pt\").to(device)\n",
        "      outputs = model(**encoding)\n",
        "      scores[text] = outputs.logits[0, :].item()\n",
        "\n",
        "  if scores[texts[0]] > scores[texts[1]]:\n",
        "    acc += 1"
      ],
      "metadata": {
        "id": "y8IGvKyadXed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calcualte the final accuracy\n",
        "\n",
        "print(acc)\n",
        "print(len(img_list)-4)\n",
        "print('The accuracy is: ', acc/(len(img_list)-4)*100) # substitute the grayscale images"
      ],
      "metadata": {
        "id": "dQ1oez2xdqIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BridgeTower Model"
      ],
      "metadata": {
        "id": "7a_Z2NgyHa0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "processor = BridgeTowerProcessor.from_pretrained(\"BridgeTower/bridgetower-base-itm-mlm\")\n",
        "model = BridgeTowerForImageAndTextRetrieval.from_pretrained(\"BridgeTower/bridgetower-base-itm-mlm\")\n",
        "model.to(device)\n",
        "acc = 0\n",
        "\n",
        "for i in tqdm(range(len(img_list))):\n",
        "  url = img_list[i]\n",
        "  if i == 550 or i == 1164 or i==1752 or i ==2236: # these are greyscale images. we are removing them\n",
        "    continue\n",
        "  image = Image.open(requests.get(url, stream=True).raw)\n",
        "  texts = [foil_list[i], cap_list[i]]\n",
        "\n",
        "  # forward pass\n",
        "  scores = dict()\n",
        "  for text in texts:\n",
        "      # prepare inputs\n",
        "      encoding = processor(image, text, return_tensors=\"pt\").to(device)\n",
        "      outputs = model(**encoding)\n",
        "      scores[text] = outputs.logits[0, 1].item()\n",
        "\n",
        "  if scores[texts[0]] < scores[texts[1]]:\n",
        "    acc += 1"
      ],
      "metadata": {
        "id": "zbogCsw8ERl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calcualte the final accuracy\n",
        "\n",
        "print(acc)\n",
        "print(len(img_list)-4)\n",
        "print('The accuracy is: ', acc/(len(img_list)-4)*100) # substitute the grayscale images"
      ],
      "metadata": {
        "id": "xXV1CmyZD1dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if there exists any greyscale image in MS COCO val set. We need to remove them due to incompatibility issues\n",
        "# if there is any grayscale image, the code will provide error message\n",
        "# only run once\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "for i in tqdm(range(2237, 2511)):\n",
        "  url = img_list[i]\n",
        "\n",
        "  img = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "  img.split()\n",
        "  ### splitting b,g,r channels\n",
        "  r,g,b=img.split()\n",
        "\n",
        "  ### PIL to numpy conversion:\n",
        "  r = np.array(r)\n",
        "  g = np.array(g)\n",
        "  b = np.array(b)\n",
        "\n",
        "  ### getting differences between (b,g), (r,g), (b,r) channel pixels\n",
        "  r_g=np.count_nonzero(abs(r-g))\n",
        "  r_b=np.count_nonzero(abs(r-b))\n",
        "  g_b=np.count_nonzero(abs(g-b))\n",
        "\n",
        "  ### sum of differences\n",
        "  diff_sum=float(r_g+r_b+g_b)\n",
        "\n",
        "  ### get image size:\n",
        "  width, height = img.size\n",
        "\n",
        "  ### get total pixels on image:\n",
        "  totalPixels = width * height\n",
        "\n",
        "  ### finding ratio of diff_sum with respect to size of image\n",
        "  ratio = diff_sum/totalPixels\n"
      ],
      "metadata": {
        "id": "WLE8t9yFGzEE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}